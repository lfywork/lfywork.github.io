
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Neural network optimization · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="../chapter1/5.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Chapter 1 Linux c</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../chapter1/1.html">
            
                <a href="../chapter1/1.html">
            
                    
                    Multiple process programming
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../chapter1/2.html">
            
                <a href="../chapter1/2.html">
            
                    
                    Environment Setting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../chapter1/3.html">
            
                <a href="../chapter1/3.html">
            
                    
                    The use of libxml2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../chapter1/4.html">
            
                <a href="../chapter1/4.html">
            
                    
                    键值对file操作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../chapter1/5.html">
            
                <a href="../chapter1/5.html">
            
                    
                    文件操作
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Chapter 2 Tensorflow</li>
        
        
    
        <li class="chapter active" data-level="2.1" data-path="4.html">
            
                <a href="4.html">
            
                    
                    Neural network optimization
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Neural network optimization</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="4--neural-network-optimization">4  Neural network optimization</h2>
<h3 id="41-concept">4.1 concept</h3>
<p>Neuron model:  its mathematical formula(activation functions): </p>
<p><script type="math/tex; mode=display">
f(\sum_{i}^n x_iw_i+b).
</script></p>
<p>Commonly used activation functions are relu(), sigmoid(), tanh().</p>
<p>Tf interface: </p>
<ol>
<li>tf.nn.relu()</li>
</ol>
<p><script type="math/tex; mode=display">
    f(x)=max(x, 0)=
    \begin{cases}
    0, & \text{x<0} \\
    x, & \text{x>=0}
    \end{cases}.
</script></p>
<ol>
<li>tf.nn.sigmoid()</li>
</ol>
<p><script type="math/tex; mode=display">
f(x)=\frac{1}{1+e^{-x}}.
</script></p>
<ol>
<li>tf.nn.tanh()</li>
</ol>
<p><script type="math/tex; mode=display">
f(x)=\frac{1-e^{-2x}}{1+e^{-2z}}.
</script></p>
<blockquote>
<p>The complexity of neural networks: It relate to the number of layers of neural network and the number of parameters to be optimized in the neural network.</p>
<p>The number of layers of the neural network = n (hidden layers) + 1(output layer)
(Attention: not include input layer usually)</p>
<p>The number of parameters to be optimized = number of &#x2018;w&#x2019; + number of &#x2018;b&#x2019;</p>
</blockquote>
<p>Loss function: mean square error(MSE), custom, cross entropy</p>
<p>1) mse:
<script type="math/tex; mode=display">
  MSE(y_0, y)=\frac{\sum_{i=1}^n (y-y_0)^2}{n}.
</script>
loss_mse = tf.reduce_mean(tf.square(y - y0))</p>
<p>2) custom:<br>for example:
<script type="math/tex; mode=display">
  loss=\sum_{}^n f(y, y_0).
</script></p>
<p><script type="math/tex; mode=display">
  f(y, y_0)=
  \begin{cases}
  PROFIT*(y_0-y) &y<y0 \\
  COST*(y-y_0) &y>=y_0
  \end{cases}.
</script>
loss = tf.reduce_sum(tf.where(tf.greater(y,y0), COST <em> (y-y0), PROFIT </em> (y0-y)))</p>
<p>3) cross entropy&#x4EA4;&#x53C9;&#x71B5;:
<script type="math/tex; mode=display">
  H(y_0, y)=-\sum y_ 0*\log{y}
</script></p>
<p>ce= -tf.reduce<em>mean(y</em>* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))</p>
<p>softmax &#x51FD;&#x6570;:&#x5C06; n &#x5206;&#x7C7B;&#x7684; n &#x4E2A;&#x8F93;&#x51FA;(y1,y2...yn)&#x53D8;&#x4E3A;&#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x6982;&#x7387;&#x5206;&#x5E03;&#x8981;&#x6C42;&#x7684;&#x51FD;&#x6570;&#x3002;</p>
<p><script type="math/tex; mode=display">
\forall x,{\bf {P}}({\bf {X}}=x)\in [0,1]\\ \sum {\bf {P}}({\bf {X}}=x)=1
</script></p>
<p>&#x2003;softmax &#x51FD;&#x6570;&#x8868;&#x793A;&#x4E3A;:</p>
<p><script type="math/tex; mode=display">
softmax(y_i)=\frac{e^{y_i}}{\sum_{j=1}^n e^{y_i}}
</script></p>
<p>softmax &#x51FD;&#x6570;&#x5E94;&#x7528;:&#x5728; n &#x5206;&#x7C7B;&#x4E2D;,&#x6A21;&#x578B;&#x4F1A;&#x6709; n &#x4E2A;&#x8F93;&#x51FA;,&#x5373; y1,y2...yn,&#x5176;&#x4E2D; yi &#x8868;&#x793A;&#x7B2C; i &#x79CD;&#x60C5;&#x51B5;&#x51FA;&#x73B0;&#x7684;&#x53EF;
&#x80FD;&#x6027;&#x5927;&#x5C0F;&#x3002;&#x5C06; n &#x4E2A;&#x8F93;&#x51FA;&#x7ECF;&#x8FC7; softmax &#x51FD;&#x6570;,&#x53EF;&#x5F97;&#x5230;&#x7B26;&#x5408;&#x6982;&#x7387;&#x5206;&#x5E03;&#x7684;&#x5206;&#x7C7B;&#x7ED3;&#x679C;&#x3002;
&#x5728; Tensorflow &#x4E2D;,&#x4E00;&#x822C;&#x8BA9;&#x6A21;&#x578B;&#x7684;&#x8F93;&#x51FA;&#x7ECF;&#x8FC7; sofemax &#x51FD;&#x6570;,&#x4EE5;&#x83B7;&#x5F97;&#x8F93;&#x51FA;&#x5206;&#x7C7B;&#x7684;&#x6982;&#x7387;&#x5206;&#x5E03;,&#x518D;&#x4E0E;&#x6807;&#x51C6;
&#x7B54;&#x6848;&#x5BF9;&#x6BD4;,&#x6C42;&#x51FA;&#x4EA4;&#x53C9;&#x71B5;,&#x5F97;&#x5230;&#x635F;&#x5931;&#x51FD;&#x6570;,&#x7528;&#x5982;&#x4E0B;&#x51FD;&#x6570;&#x5B9E;&#x73B0;:</p>
<pre><code>ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
cem = tf.reduce_mean(ce)
</code></pre><h3 id="42-learning-rate">4.2 Learning Rate</h3>
<p>Learning rate</p>
<p><script type="math/tex; mode=display">
\theta_{n+1}=\theta_n -\alpha\frac{\partial J(\theta_n)}{\partial\theta_n},J(\theta_n)\text{ is loss function. } \alpha\text{ is learning rate.}
</script></p>
<p>I. &#x201C;Debugging&#x201D;: How to make sure gradient descent is working correctly.</p>
<p>II. How to choose learning rate &#x3B1;.</p>
<p>If  &#x3B1; is too small, slow convergence. Declare convergence if J(&#x3B8;) decreases by less than 10-3 in one iteration.
If  &#x3B1; is too large, J(&#x3B8;) may not decrease on every iteration, may not converge.</p>
<p>&#x5B66;&#x4E60;&#x7387;&#x8FC7;&#x5927;,&#x4F1A;&#x5BFC;&#x81F4;&#x5F85;&#x4F18;&#x5316;&#x7684;&#x53C2;&#x6570;&#x5728;&#x6700;&#x5C0F;&#x503C;&#x9644;&#x8FD1;&#x6CE2;&#x52A8;,&#x4E0D;&#x6536;&#x655B;;&#x5B66;&#x4E60;&#x7387;&#x8FC7;&#x5C0F;,&#x4F1A;&#x5BFC;&#x81F4;&#x5F85;&#x4F18;&#x5316;&#x7684;&#x53C2;&#x6570;&#x6536;&#x655B;&#x7F13;&#x6162;&#x3002;</p>
<p>J(&#x3B8;)  should decrease after every iteration.1)For sufficiently &#x3B1; small ,J(&#x3B8;) should decrease after every iteration.2)But if &#x3B1; is too small, gradient descent can be slow to converge.
It&#x2019;s too difficult to choose the suitable &#x3B1;. We have to try the best &#x3B1;.</p>
<p>E.g. try range of number:
0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1</p>
<p>E.g.
Index attenuation learning rate&#x6307;&#x6570;&#x8870;&#x51CF;&#x5B66;&#x4E60;&#x7387;:The learning rate is updated dynamically with the change of the number of training wheels</p>
<p><script type="math/tex; mode=display">
\alpha(T)=\frac{\alpha_0*T*\text{LEARNING_RATE_DECAY}}{\text{LEARNING_RATE_STEP}}
</script></p>
<pre><code>T = tf.Variable(0, trainable=False)
learning_rate = tf.train.exponential_decay(
                &#x3B1;_0,
                T,
                LEARNING_RATE_STEP, LEARNING_RATE_DECAY,
                staircase=True/False)
</code></pre><blockquote>
<p>&#x5176;&#x4E2D;, &#x82E5; staircase &#x8BBE;&#x7F6E;&#x4E3A; True &#x65F6;,&#x8868;&#x793A; global_step/learning rate step &#x53D6;&#x6574;&#x6570;,&#x5B66;&#x4E60;&#x7387;&#x9636;&#x68AF;&#x578B;&#x8870;&#x51CF;;&#x82E5; staircase &#x8BBE;&#x7F6E;&#x4E3A; false &#x65F6;,&#x5B66;&#x4E60;&#x7387;&#x4F1A;&#x662F;&#x4E00;&#x6761;&#x5E73;&#x6ED1;&#x4E0B;&#x964D;&#x7684;&#x66F2;&#x7EBF;&#x3002;</p>
</blockquote>
<h3 id="43-moving-average">4.3 Moving Average</h3>
<p>Moving Average: The average values of all parameters w and B in a model are recorded over a period of time. The generalization ability of the model can be enhanced by using the moving average.</p>
<h3 id="44-overfitting-and-regularization">4.4 Overfitting and Regularization</h3>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../chapter1/5.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: 文件操作">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Neural network optimization","level":"2.1","depth":1,"previous":{"title":"文件操作","level":"1.6","depth":1,"path":"chapter1/5.md","ref":"chapter1/5.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["mathjax","livereload"],"pluginsConfig":{"mathjax":{"forceSVG":false,"version":"2.6-latest"},"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"chapter2/4.md","mtime":"2018-05-25T17:57:11.867Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2018-05-25T17:51:33.489Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

