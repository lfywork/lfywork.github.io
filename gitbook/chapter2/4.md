## 4  Neural network optimization
### 4.1 concept

Neuron model:  its mathematical formula(activation functions): 

$$
f(\sum_{i}^n x_iw_i+b).
$$

Commonly used activation functions are relu(), sigmoid(), tanh().

Tf interface: 
1. tf.nn.relu()

$$
    f(x)=max(x, 0)=
    \begin{cases}
    0, & \text{x<0} \\
    x, & \text{x>=0}
    \end{cases}.
$$
    
2. tf.nn.sigmoid()

$$
f(x)=\frac{1}{1+e^{-x}}.
$$

3. tf.nn.tanh()

$$
f(x)=\frac{1-e^{-2x}}{1+e^{-2z}}.
$$

                                                                                                                                                                
>The complexity of neural networks: It relate to the number of layers of neural network and the number of parameters to be optimized in the neural network.

>The number of layers of the neural network = n (hidden layers) + 1(output layer)
(Attention: not include input layer usually)

>The number of parameters to be optimized = number of ‘w’ + number of ‘b’
                                                                                                                                                               
Loss function: mean square error(MSE), custom, cross entropy

1) mse:
$$
  MSE(y_0, y)=\frac{\sum_{i=1}^n (y-y_0)^2}{n}.
$$
loss_mse = tf.reduce_mean(tf.square(y - y0))

2) custom:	
for example:
$$
  loss=\sum_{}^n f(y, y_0).
$$

$$
  f(y, y_0)=
  \begin{cases}
  PROFIT*(y_0-y) &y<y0 \\
  COST*(y-y_0) &y>=y_0
  \end{cases}.
$$
loss = tf.reduce_sum(tf.where(tf.greater(y,y0), COST * (y-y0), PROFIT * (y0-y)))

3) cross entropy交叉熵:
$$
  H(y_0, y)=-\sum y_ 0*\log{y}
$$

ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))

softmax 函数:将 n 分类的 n 个输出(y1,y2...yn)变为满足以下概率分布要求的函数。

$$
\forall x,{\bf {P}}({\bf {X}}=x)\in [0,1]\\ \sum {\bf {P}}({\bf {X}}=x)=1
$$

&emsp;softmax 函数表示为:

$$
softmax(y_i)=\frac{e^{y_i}}{\sum_{j=1}^n e^{y_i}}
$$

softmax 函数应用:在 n 分类中,模型会有 n 个输出,即 y1,y2...yn,其中 yi 表示第 i 种情况出现的可
能性大小。将 n 个输出经过 softmax 函数,可得到符合概率分布的分类结果。
在 Tensorflow 中,一般让模型的输出经过 sofemax 函数,以获得输出分类的概率分布,再与标准
答案对比,求出交叉熵,得到损失函数,用如下函数实现:

	ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
	cem = tf.reduce_mean(ce)

### 4.2 Learning Rate

Learning rate

$$
\theta_{n+1}=\theta_n -\alpha\frac{\partial J(\theta_n)}{\partial\theta_n},J(\theta_n)\text{ is loss function. } \alpha\text{ is learning rate.}
$$

I. “Debugging”: How to make sure gradient descent is working correctly.

II. How to choose learning rate α.

If  α is too small, slow convergence. Declare convergence if J(θ) decreases by less than 10-3 in one iteration.
If  α is too large, J(θ) may not decrease on every iteration, may not converge.

学习率过大,会导致待优化的参数在最小值附近波动,不收敛;学习率过小,会导致待优化的参数收敛缓慢。

J(θ)  should decrease after every iteration.1)For sufficiently α small ,J(θ) should decrease after every iteration.2)But if α is too small, gradient descent can be slow to converge.
It’s too difficult to choose the suitable α. We have to try the best α.

E.g. try range of number:
0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1

E.g.
Index attenuation learning rate指数衰减学习率:The learning rate is updated dynamically with the change of the number of training wheels

$$
\alpha(T)=\frac{\alpha_0*T*\text{LEARNING_RATE_DECAY}}{\text{LEARNING_RATE_STEP}}
$$

	T = tf.Variable(0, trainable=False)
	learning_rate = tf.train.exponential_decay(
					α_0,
					T,
					LEARNING_RATE_STEP, LEARNING_RATE_DECAY,
					staircase=True/False)
>其中, 若 staircase 设置为 True 时,表示 global_step/learning rate step 取整数,学习率阶梯型衰减;若 staircase 设置为 false 时,学习率会是一条平滑下降的曲线。

### 4.3 Moving Average

Moving Average: The average values of all parameters w and b in a model are recorded over a period of time. The generalization ability of the model can be enhanced by using the moving average.

Moving Avuerage Value: 

$$
影子 = 衰减率*影子 + (1 - 衰减率)*参数 \\
其中，衰减率=min\{\text{MOVING_AVERAGE_DECAY}, \frac{1+轮数}{10+轮数}\},影子初值=参数初值
$$

tf function:

	1.
	ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
	// MOVING_AVERAGE_DECAY表示滑动平均衰减率,一般会赋接近 1 的值,global_step 表示当前训练了多少轮
	2.
	ema_op = ema.apply(tf.trainable_variables())
	// 其中,ema.apply()函数实现对括号内参数求滑动平均,tf.trainable_variables()函数实现把所有待训练参数汇总为列表
	3.
	with tf.control_dependencies([train_step, ema_op]):
	train_op = tf.no_op(name='train')
	// 其中,该函数实现将滑动平均和训练过程同步运行。
	
	// 查看模型中参数的平均值,可以用 ema.average()函数。

### 4.4 Overfitting and Regularization

